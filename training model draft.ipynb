{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import torch #keep\nimport torchvision #keep\nimport torchvision.transforms as transforms   #best practice, get rid of line and expand explicity all transforms references?\nimport pandas\nimport os",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "class customSet(torch.utils.data.Dataset):\n\n    # save annotations csv data and dataset directory\n    def __init__(self, annotations_file, audio_dir, transformation = NONE, target_sample_rate, num_samples):\n        self.annotations = pandas.read_csv(annotations_file)\n        self.audio_dir = audio_dir\n        self.transformation = transformation\n        self.target_sample_rate = target_sample_rate\n        self.num_samples = num_samples\n\n    \n    # number of items in dataset (one item per line in annotations csv data)\n    def __len__(self):\n        return len(self.annotations)\n\n    \n    # retrieve signal and label of a given index of item\n    def __getitem__(self, index):\n        audio_sample_path = self._get_audio_sample_path(index)\n        # use .load_wav instead?\n        signal, sample_rate = torchaudio.load(audio_sample_path)\n\n        # TODO: necesary? if we already know how many channels and sample rates we're training /testing with...\n        # function: wrap in \n        signal = self._resample_if_necessary(signal)                \n        signal = self._mix_down_if_necessary(signal)\n        signal = self._cut_if_necesary(signal)\n        signal = self._right_pad_if_necessary(signal)\n\n        \n        # TODO: does this line need to be wrapped in IF-statement if .transformation = NONE \n        signal = self.transformation(signal)\n        \n        label = self._get_audio_label(index)\n        return signal, label\n\n    \n    # returns path for given index of item\n    def _get_audio_sample_path(self, index):\n        # TODO confirm configuration of title/where to retrieve in dataset directory\n        title = self.annotations.iloc[index, 0]\n        genre = self.annotations.iloc[index, 1]\n        \n        # this line is for stringing together folder path with the info retrieved from iloc\n        path = os.path.join(self.audio_dir, outerfolder , title)\n\n        return path\n\n    \n    # if target sample rate greater than desired sample_rate then resample\n    def _resample_if_necessary(self, signal, sample_rate):\n        if sample_rate != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(sample_rate, self.target_sample_rate)\n            signal = resampler(signal)\n        return signal\n\n    \n    # if more than 1 channel, mixex signal down to mono\n    def _mix_down_if_necessary(self, signal):\n        # shape[0] gives channels, \n        if self.shape[0] > 1 :\n            signal = torch.mean(signal, dim = 0, keepdim = True)\n        return signal\n\n    \n    # if length of signal greater than num_samples, cut signal    \n    def _cut_if_necesary(self, signal):\n        # signal is Tensor that is a tuple of (numchannels, numsamples) -> since mixed down prior, then expected (1, num_samples), just check .shape[1]\n        if signal.shape[1] > signal.num_samples:\n            signal = signal[:, : self.num_samples]\n        return\n\n\n    # if length of signal less than num_samples, pad with zeros\n    def _right_pad_if_necessary(self, signal):\n        signal_length = signal.shape[1]\n        if signal_length < signal.num_samples:\n            num_missing_samples = signal.num_samples - signal_length\n            last_dim_padding = (0, num_missing_samples)\n            signal = torch.nn.functional.pad(signal, last_dim_padding)\n        return signal\n   \n    \n    # returns label given index of item\n    def _get_audio_label(self, index):\n        return self.annotations.iloc[index, 1]\n    \n\nif __name__ == \"__main__\":\n    # TODO: add hardcoded local file path  \n    ANNOTATIONS_FILE = \n    AUDIO_DIR = \n    SAMPLE_RATE = # 22050\n    NUM-SAMPLES = # 22050\n    # create mel spectrogram object\n    mel_spec = torchaudio.transforms.MelSpectrogram(\n        sample_rate = SAMPLE_RATE,\n        n_fft = 1024,\n        hop_length = 512,\n        n_mels = 64\n    )\n        \n    mydatainstance =  customSet(ANNOTATIONS_FILE, AUDIO_DIR, mel_spec, num_samples)\n\n    signal, label = mydatainstance[0]\n    ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "\nbatch_size = 32\n\n# TODO: change sets to custom dataset; no image transform required? since using audio...?\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\n\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=4)\n\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=4)\n\n\n# ?\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "import os\nimport pandas \nfrom torchvision.io import read_image\n\nclass CustomImageDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n        self.img_labels = pandas.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n        image = read_image(img_path)\n        label = self.img_labels.iloc[idx, 1]\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            label = self.target_transform(label)\n        return image, label",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "from tabulate import tabulate\n\nprint('Training set')\nprint(f'Samples: {trainset.data.shape}')\nprint(f'Labels: {len(trainset.targets)}')\n\nprint('\\nTest set')\nprint(f'Samples: {testset.data.shape}')\nprint(f'Labels: {len(testset.targets)}')\n\nprint('\\nClasses\\n')\nprint(tabulate(\n    list(trainset.class_to_idx.items()), headers=['Name', 'Index'], \n    tablefmt='orgtbl'\n))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# functions to show an image\n\n\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\n# get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# show images\nimshow(torchvision.utils.make_grid(images))\n# print labels\nprint(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "torch.cuda.is_available()\ntorch.cuda.get_device_name(0)\nprint(torch.cuda.get_device_name(0))\n\n#keep\nif torch.cuda.is_available(): \n dev = \"cuda:0\" \nelse: \n dev = \"cpu\" \n\n#dev = \"cpu\" \ndevice = torch.device(dev) \ndev = \"cuda\" \nprint(device)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "import torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\n\n#keep\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3,32,5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(32, 32, 5)\n        self.fc1 = nn.Linear(32 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nnet = Net()\nnet.to(device)\nprint(net)\n\nsummary(net, (3,32,32), batch_size=32, device=dev)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "%%time\nfor epoch in range(20):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n        inputs, labels = inputs.to(device,non_blocking=True), labels.to(device, non_blocking=True)\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        \n        if i % 400 == 399:    # print every 400 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 2000))\n            running_loss = 0.0\n\nprint('Finished Training')\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "dataiter = iter(testloader)\nimages, labels = next(dataiter)\n\n# print images\nimshow(torchvision.utils.make_grid(images))\nprint('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(8)))\n\nimages, labels = next(dataiter)\n\n# print images\nimshow(torchvision.utils.make_grid(images))\nprint('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(8)))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "PATH = './cifar_net.pth'\ntorch.save(net.state_dict(), PATH)\n\nprint(device)\nnet = Net()\nnet.load_state_dict(torch.load(PATH))\nnet.to(device)\nimages = images.to(device)\noutputs = net(images)\n_, predicted = torch.max(outputs, 1)\n\n# print(outputs)\nprint('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n                              for j in range(8)))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def test_accuracy(net, testloader, device):\n    correct = 0\n\n    # since we're not training, we don't need to calculate the gradients for our outputs\n    with torch.no_grad():\n        net.eval()\n        for images, labels in testloader:\n            images, labels = images.to(device), labels.to(device)\n            #  = images + 0.2 * torch.randn(images.shape).to(device)\n            \n            # calculate outputs by running images through the network\n            outputs = net(images)\n\n            # the class with the highest energy is what we choose as prediction\n            predicted = torch.max(outputs.data, 1)[1]\n\n            correct += (predicted == labels).sum().item()\n    \n    return correct / len(testloader.dataset)\n\ndef test_accuracy_per_class(net, testloader, device):\n    correct_pred = {classname: 0 for classname in trainset.classes}\n    total_pred = {classname: 0 for classname in trainset.classes}\n\n    with torch.no_grad():\n        net.eval()\n        for images, labels in testloader:\n            images, labels = images.to(device), labels.to(device)\n\n            outputs = net(images)\n            predicted = torch.max(outputs.data, 1)[1]\n\n            # collect the correct predictions for each class\n            for label, prediction in zip(labels, predicted):\n                if label == prediction:\n                    correct_pred[trainset.classes[label]] += 1\n                total_pred[trainset.classes[label]] += 1\n    \n    accuracy_per_class = {classname: 0 for classname in trainset.classes}\n    for classname, correct_count in correct_pred.items():\n        accuracy = (100 * float(correct_count)) / total_pred[classname]\n        accuracy_per_class[classname] = accuracy\n\n    return accuracy_per_class\n\ntest_acc = test_accuracy(net, testloader, 'cuda')\nprint(f'Best trial test set accuracy: {test_acc}')\n\noverall_accuracy = test_accuracy(net, testloader, dev)\n\nprint(\n    'Overall accuracy of the network  '\n    f'{(overall_accuracy * 100):.2f} %\\n'\n    'on the 10000 test images'\n)\n\naccuracy_per_class = test_accuracy_per_class(net, testloader, dev)\n\nprint('Accuracy per class\\n')\nfor classname, accuracy in accuracy_per_class.items():\n    print(f'{classname:12s} {accuracy:.2f} %')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\n\n# Import datasets, classifiers and performance metrics\nfrom sklearn import datasets, metrics, svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report\n\nprint('Training set')\nprint(f'Samples: {trainset.data.shape}')\nprint(f'Labels: {len(trainset.targets)}')\nprint(type(trainset.data))\nprint(type(trainset.targets))\nprint('\\nTest set')\nprint(f'Samples: {testset.data.shape}')\nprint(f'Labels: {len(testset.targets)}')\n\ntrain_n_samples = len(trainset.data)\nprint(train_n_samples)\ntest_n_samples = len(testset.data)\nprint(test_n_samples)\n\nXtrain = trainset.data\nXtest  = testset.data\n\nfrom skimage.feature import hog\nXtrain_hog = []\nfor i in range(len(Xtrain)):\n    fd  = hog(Xtrain[i] , orientations=9 , pixels_per_cell = (8,8),\n                     cells_per_block = (2,2) , visualize = False, channel_axis=-1)\n    Xtrain_hog.append(fd)\n    if ((i % 10000) == 0): \n        print(i)\n\nXtrain_hog = np.array(Xtrain_hog)\nprint('Done calculating HOGs for training')\nprint(Xtrain_hog.shape)\n\nXtest_hog = []\nfor i in range(len(Xtest)):\n    fd = hog(Xtest[i] , orientations=9 , pixels_per_cell = (8,8),\n                     cells_per_block = (2,2) , visualize = False, channel_axis=-1)\n    Xtest_hog.append(fd)\n    if ((i % 1000) == 0): \n        print(i)\n\nXtest_hog = np.array(Xtest_hog)\nprint('Done calculating HOGs for testing')\n\nytrain = trainset.targets\nytest  = testset.targets\n\nfrom sklearn.decomposition import PCA\npca = PCA(0.8)\nXtrain_pca = pca.fit_transform(Xtrain_hog)\nXtest_pca  = pca.transform(Xtest_hog)\nprint(Xtrain_pca.shape)\nprint(Xtest_pca.shape)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "%%time \n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n# Create a classifier: a support vector classifier\nclf = svm.SVC(C=10, cache_size=10000)\nclf.fit(Xtrain_pca, ytrain)\n\nytest_predict  = clf.predict(Xtest_pca)\nprint(classification_report(ytest, ytest_predict))\n\ncolor = 'white'\ncm = confusion_matrix(ytest, ytest_predict)\ndisp = ConfusionMatrixDisplay(cm, display_labels=['airplanes', 'cars', 'birds', 'cats', 'deer', 'dogs', 'frogs', 'horses', 'ships', 'trucks'])\ndisp.plot()\nplt.xticks(rotation=45, ha='right')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}